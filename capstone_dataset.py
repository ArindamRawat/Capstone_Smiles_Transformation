# -*- coding: utf-8 -*-
"""Capstone_Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tckRkhursW-z9gib8P3S-A4JcvcB2cNl
"""

!pip install rdkit

!pip install pubchempy

from rdkit import Chem
from rdkit.Chem.Draw import IPythonConsole

mol = Chem.MolFromSmiles('CCO')
mol

import pubchempy as pcp
import pandas as pd
from tqdm import tqdm
from rdkit import Chem

valid_smiles = []

for cid in tqdm(range(1, 50000)):
    try:
        c = pcp.Compound.from_cid(cid)
        smi = c.isomeric_smiles

        mol = Chem.MolFromSmiles(smi)
        if mol:
            cano = Chem.MolToSmiles(mol, canonical=True)
            valid_smiles.append(cano)

    except:
        continue

df = pd.DataFrame(valid_smiles, columns=["smiles"])
df.to_csv("valid_smiles.csv", index=False)
print(len(df), "valid SMILES saved!")

import random
import re
import pandas as pd
from tqdm import tqdm
from rdkit import Chem
import os

random.seed(42)

INPUT_CSV = "valid_smiles.csv"
OUT_CSV = "smiles_correction_dataset.csv"
TRAIN_CSV = "train_pairs.csv"
VAL_CSV = "val_pairs.csv"
TEST_CSV = "test_pairs.csv"

CORRUPTIONS_PER_SMILES = 3
MAX_ROWS = None
MAX_CORRUPTED_LENGTH = 1000
SMILES_TOKEN_PATTERN = re.compile(
    r"Cl|Br|\[[^\]]+\]|Si|Se|B|C|N|O|P|S|F|I|H|n|c|o|s|@|:|%[0-9]{2}|\d+|=|#|-|/|\\\\|\(|\)|\.|\+|\\-"
)

def tokenize_smiles(smi: str):
    if not smi:
        return []
    tokens = SMILES_TOKEN_PATTERN.findall(smi)
    if tokens:
        return [t for t in tokens if t]
    else:
        return list(smi)

def corrupt_smiles(smi: str) -> str:
    if smi is None:
        return ""
    tokens = tokenize_smiles(smi)
    if len(tokens) == 0:
        tokens = list(smi)

    p_delete = 0.25
    p_insert = 0.25
    p_substitute = 0.25
    p_swap = 0.15
    p_shuffle_segment = 0.05

    if len(tokens) > 0 and random.random() < p_delete:
        idx = random.randrange(0, len(tokens))
        del tokens[idx]

    if random.random() < p_insert:
        insert_token = random.choice(['C','N','O','S','P','Cl','Br','(',')','=','1','2'])
        idx = random.randrange(0, len(tokens) + 1)
        tokens.insert(idx, insert_token)

    if len(tokens) > 0 and random.random() < p_substitute:
        idx = random.randrange(0, len(tokens))
        replacement = random.choice(['C','N','O','S','P','H','Cl','Br','(',')','=','1','2'])
        tokens[idx] = replacement

    if len(tokens) > 1 and random.random() < p_swap:
        i, j = random.sample(range(len(tokens)), 2)
        tokens[i], tokens[j] = tokens[j], tokens[i]

    if len(tokens) > 2 and random.random() < p_shuffle_segment:
        start = random.randrange(0, len(tokens)-1)
        end = min(len(tokens), start + random.randint(2, min(4, len(tokens)-start)))
        seg = tokens[start:end]
        random.shuffle(seg)
        tokens[start:end] = seg

    corrupted = "".join(tokens)
    if corrupted == smi:
        if len(tokens) > 0:
            tokens = tokens[:-1]
            corrupted = "".join(tokens)
        else:
            corrupted = smi + "%"

    if len(corrupted) > MAX_CORRUPTED_LENGTH:
        corrupted = corrupted[:MAX_CORRUPTED_LENGTH]

    return corrupted

def canonicalize_smiles(smi):
    try:
        mol = Chem.MolFromSmiles(smi)
        if mol is None:
            return None
        return Chem.MolToSmiles(mol, canonical=True)
    except:
        return None

if not os.path.exists(INPUT_CSV):
    raise FileNotFoundError(f"{INPUT_CSV} not found in current directory.")

df = pd.read_csv(INPUT_CSV)
if "smiles" not in df.columns:
    df.columns = ["smiles"] if df.shape[1] == 1 else df.columns

canonical_list = []
seen = set()
for smi in tqdm(df["smiles"].astype(str).tolist(), desc="Canonicalizing"):
    can = canonicalize_smiles(smi)
    if can and can not in seen:
        seen.add(can)
        canonical_list.append(can)
    if MAX_ROWS and len(canonical_list) >= MAX_ROWS:
        break

print(f"Canonical SMILES count: {len(canonical_list)}")

out_cols = ["corrupted", "correct"]
pd.DataFrame(columns=out_cols).to_csv(OUT_CSV, index=False)  # header

total_pairs = 0
for can_smi in tqdm(canonical_list, desc="Generating corrupted pairs"):
    row_pairs = []
    for _ in range(CORRUPTIONS_PER_SMILES):
        corrupted = corrupt_smiles(can_smi)
        row_pairs.append([corrupted, can_smi])
    pd.DataFrame(row_pairs, columns=out_cols).to_csv(OUT_CSV, mode="a", header=False, index=False)
    total_pairs += len(row_pairs)

print(f"Wrote {total_pairs} pairs to {OUT_CSV}")

pairs_df = pd.read_csv(OUT_CSV)
pairs_df.drop_duplicates(inplace=True)
print("After dedup, pairs:", len(pairs_df))

unique_canon = pairs_df["correct"].unique().tolist()
random.shuffle(unique_canon)

n = len(unique_canon)
n_train = int(0.70 * n)
n_val = int(0.20 * n)
train_can = set(unique_canon[:n_train])
val_can = set(unique_canon[n_train:n_train + n_val])
test_can = set(unique_canon[n_train + n_val:])

train_df = pairs_df[pairs_df["correct"].isin(train_can)].reset_index(drop=True)
val_df = pairs_df[pairs_df["correct"].isin(val_can)].reset_index(drop=True)
test_df = pairs_df[pairs_df["correct"].isin(test_can)].reset_index(drop=True)

print("Train pairs:", len(train_df))
print("Val pairs:", len(val_df))
print("Test pairs:", len(test_df))

train_df.to_csv(TRAIN_CSV, index=False)
val_df.to_csv(VAL_CSV, index=False)
test_df.to_csv(TEST_CSV, index=False)
print("Saved train/val/test files.")

def rdkit_validity_rate(series):
    ok = 0
    for s in series:
        if Chem.MolFromSmiles(str(s)) is not None:
            ok += 1
    return ok / len(series) if len(series) > 0 else 0

print("Original canonical valid SMILES (should parse):", rdkit_validity_rate(canonical_list))
print("Corrupted (how many still parse):", rdkit_validity_rate(pairs_df["corrupted"].tolist()))
print("Correct (parses):", rdkit_validity_rate(pairs_df["correct"].tolist()))

print("\nRandom sample pairs:")
print(train_df.sample(5))

import pandas as pd

TRAIN_CSV = "train_pairs.csv"
VAL_CSV   = "val_pairs.csv"
TEST_CSV  = "test_pairs.csv"

train = pd.read_csv(TRAIN_CSV)
val   = pd.read_csv(VAL_CSV)
test  = pd.read_csv(TEST_CSV)

n_train = len(train)
n_val   = len(val)
n_test  = len(test)
n_total = n_train + n_val + n_test

print(f"Pairs — Train: {n_train}, Val: {n_val}, Test: {n_test}, Total: {n_total}")
print(f"Percentages — Train: {n_train/n_total:.3%}, Val: {n_val/n_total:.3%}, Test: {n_test/n_total:.3%}")

train_mols = set(train['correct'].unique())
val_mols   = set(val['correct'].unique())
test_mols  = set(test['correct'].unique())

print("Unique canonical molecules — Train:", len(train_mols))
print("Unique canonical molecules — Val:  ", len(val_mols))
print("Unique canonical molecules — Test: ", len(test_mols))

leak_train_val = train_mols & val_mols
leak_train_test = train_mols & test_mols
leak_val_test = val_mols & test_mols

print("Overlap (train ∩ val):", len(leak_train_val))
print("Overlap (train ∩ test):", len(leak_train_test))
print("Overlap (val ∩ test):", len(leak_val_test))

if len(leak_train_val) > 0:
    print("Examples overlap train/val:", list(leak_train_val)[:5])
if len(leak_train_test) > 0:
    print("Examples overlap train/test:", list(leak_train_test)[:5])
if len(leak_val_test) > 0:
    print("Examples overlap val/test:", list(leak_val_test)[:5])